{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Capsule\n",
    "\n",
    "### Learning the language as is. Sequence TF IDf for Text Feature generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "import abc\n",
    "from gensim.utils import simple_tokenize,simple_preprocess\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up things\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "# warnings.filterwarnings('ignore',category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, \n",
    "# preprocessor=None, tokenizer=None, analyzer=’word’, stop_words=None, \n",
    "# token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), max_df=1.0, min_df=1, \n",
    "# max_features=None, vocabulary=None, \n",
    "# binary=False, dtype=<class ‘numpy.float64’>, norm=’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['What are we doing here? Just trying to solve a problem! We will use capsules.',\n",
    "         'What is a capsule? Lets read about it.',\n",
    "         'Capsules are the single vectors represenation of multiple vectors.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'are',\n",
       " 'we',\n",
       " 'doing',\n",
       " 'here',\n",
       " 'Just',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'We',\n",
       " 'will',\n",
       " 'use',\n",
       " 'capsules']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'are',\n",
       " 'we',\n",
       " 'doing',\n",
       " 'here',\n",
       " 'just',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'problem',\n",
       " 'we',\n",
       " 'will',\n",
       " 'use',\n",
       " 'capsules']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is not good for us\n",
    "list(simple_tokenize(texts[0]))\n",
    "simple_preprocess(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are we doing here',\n",
       " ' Just trying to solve a problem',\n",
       " ' We will use capsules',\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['What', 'are', 'we', 'doing', 'here'],\n",
       " ['Just', 'trying', 'to', 'solve', 'a', 'problem'],\n",
       " ['We', 'will', 'use', 'capsules'],\n",
       " []]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# punct_re = re.compile('\\?|\\!|\\.',re.UNICODE)\n",
    "punct_re = re.compile('[?!.]')\n",
    "punct_re.split(texts[0])\n",
    "[i.split() for i  in punct_re.split(texts[0])]\n",
    "texts[0].split().index('are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'are',\n",
       " 'capsule',\n",
       " 'capsules',\n",
       " 'doing',\n",
       " 'here',\n",
       " 'is',\n",
       " 'it',\n",
       " 'just',\n",
       " 'lets',\n",
       " 'multiple',\n",
       " 'of',\n",
       " 'problem',\n",
       " 'read',\n",
       " 'represenation',\n",
       " 'single',\n",
       " 'solve',\n",
       " 'the',\n",
       " 'to',\n",
       " 'trying',\n",
       " 'use',\n",
       " 'vectors',\n",
       " 'we',\n",
       " 'what',\n",
       " 'will']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocabulary(texts):\n",
    "    vocab = []\n",
    "    for text in texts:\n",
    "        for token in simple_preprocess(text):\n",
    "            vocab.append(token)\n",
    "    return np.unique(vocab).tolist()\n",
    "get_vocabulary(texts)\n",
    "vocab = get_vocabulary(texts)\n",
    "vocab.index('vectors')\n",
    "len(vocab)\n",
    "dict_vocab = dict([(k,i) for i,k in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what', 'are', 'we', 'doing', 'here', 'just'],\n",
       " ['doing', 'here', 'just', 'trying', 'to', 'solve'],\n",
       " ['trying', 'to', 'solve', 'problem', 'we', 'will'],\n",
       " ['problem', 'we', 'will', 'use', 'capsules'],\n",
       " ['use', 'capsules']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_sentences(text,length,window):\n",
    "    if isinstance(text,list):\n",
    "        text_tokens = text\n",
    "    else:\n",
    "        text_tokens = text.split()\n",
    "    if len(text_tokens) <= length:\n",
    "        return text\n",
    "    else:\n",
    "        window_index = length - window\n",
    "        prev_index=0\n",
    "        texts = []\n",
    "        while True:\n",
    "            texts.append(text_tokens[prev_index:(prev_index+length)])\n",
    "            prev_index = prev_index+length-window\n",
    "            if prev_index >len(text_tokens):\n",
    "                break\n",
    "        \n",
    "        return [text for text in texts if len(text) != 0]\n",
    "    \n",
    "pad_sentences(simple_preprocess(texts[0]),6,3)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_preprocess('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.25      , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.31111111, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.33333333, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.11111111, 0.        , 0.        , 0.25      , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.2       , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.2       ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.33333333, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.25      , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.2       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.25      , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.11111111, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.11111111, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.2       ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.25      , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.11111111, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.11111111, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.2       , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.11111111, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.2       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.2       , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.25      , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.11111111,\n",
       "        0.        , 0.        , 0.        , 0.11111111, 0.        ],\n",
       "       [0.25      , 0.        , 0.2       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.53333333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.25      , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(25, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sequence_tf(texts, max_len, punct_re, preprocess =simple_preprocess, vocabulary=None,window=0):\n",
    "    tf_vec = np.zeros((len(vocabulary),max_len))\n",
    "    vocab_is_dict = isinstance(vocabulary,dict)\n",
    "    for text in texts:\n",
    "        for split_text in punct_re.split(text):\n",
    "            processed_text = preprocess(split_text)\n",
    "            l = len(processed_text)\n",
    "\n",
    "            if l <= max_len and l != 0:\n",
    "                for seq, word in enumerate(processed_text):\n",
    "                    indx = vocabulary[word] if vocab_is_dict else vocabulary.index(word)\n",
    "                    tf_vec[indx,seq] += 1/l\n",
    "\n",
    "            else: # need to check its behaviour\n",
    "                for padded_text in pad_sentences(processed_text, max_len, window=window):\n",
    "                    for seq, word in enumerate(padded_text):\n",
    "                        indx = vocabulary.index(word)\n",
    "                        tf_vec[indx,seq] += 1\n",
    "                    \n",
    "    return tf_vec\n",
    "\n",
    "\n",
    "@numba.jit\n",
    "def generate_sequence_tf_numba(texts, max_len, punct_re, preprocess =simple_preprocess, vocabulary=None,window=0):\n",
    "    tf_vec = np.zeros((len(vocabulary),max_len))\n",
    "    i=1\n",
    "    vocab_is_dict = isinstance(vocabulary,dict)\n",
    "    for text in texts:\n",
    "        for split_text in punct_re.split(text):\n",
    "            processed_text = preprocess(split_text)\n",
    "            l = len(processed_text)\n",
    "\n",
    "            if l <= max_len and l != 0:\n",
    "                for seq, word in enumerate(processed_text):\n",
    "                    indx = vocabulary[word] if vocab_is_dict else vocabulary.index(word)\n",
    "                    tf_vec[indx,seq] += 1/l\n",
    "\n",
    "            else: # need to check its behaviour\n",
    "                for padded_text in pad_sentences(processed_text, max_len, window=window):\n",
    "                    for seq, word in enumerate(padded_text):\n",
    "                        indx = vocabulary.index(word)\n",
    "                        tf_vec[indx,seq] += 1\n",
    "#         print(i)\n",
    "        i+=1\n",
    "                    \n",
    "    return tf_vec\n",
    "                \n",
    "generate_sequence_tf(texts, 10, punct_re, vocabulary=dict_vocab)\n",
    "generate_sequence_tf(texts, 10, punct_re, vocabulary=dict_vocab).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.25      , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.31111111, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.33333333, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.11111111, 0.        , 0.        , 0.25      , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.2       , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.2       ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.33333333, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.25      , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.2       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.25      , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.11111111, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.11111111, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.2       ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.25      , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.11111111, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.11111111, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.2       , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.11111111, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.2       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.2       , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.25      , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.11111111,\n",
       "        0.        , 0.        , 0.        , 0.11111111, 0.        ],\n",
       "       [0.25      , 0.        , 0.2       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.53333333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.25      , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequence_tf_numba(texts, 10, punct_re, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_arr = np.array(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44 µs ± 10.4 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vocab_arr == 'what'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.04 µs ± 51.6 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.where(vocab_arr == 'what')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.4 µs ± 361 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "find_index = np.vectorize(lambda x, y : y == x)\n",
    "%timeit find_index(vocab_arr,'what')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.5 µs ± 318 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "find_index = np.vectorize(lambda x, y : y == x)\n",
    "%timeit find_index(vocab,'what')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 µs ± 1.69 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_sequence_tf_numba(texts, 10, punct_re, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 µs ± 6.81 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_sequence_tf_numba(texts, 10, punct_re, vocabulary=dict_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.7 µs ± 762 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_sequence_tf(texts, 10, punct_re, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.7 µs ± 553 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_sequence_tf(texts, 10, punct_re, vocabulary=dict_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([(1,0),(1,0),(1,1)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         226 function calls in 0.004 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.002    0.002    0.004    0.004 <ipython-input-46-59e98ab18601>:1(generate_sequence_tf)\n",
       "        1    0.001    0.001    0.004    0.004 {built-in method builtins.exec}\n",
       "        9    0.000    0.000    0.001    0.000 utils.py:283(simple_preprocess)\n",
       "        9    0.000    0.000    0.000    0.000 utils.py:223(tokenize)\n",
       "       41    0.000    0.000    0.000    0.000 utils.py:265(simple_tokenize)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.zeros}\n",
       "        9    0.000    0.000    0.000    0.000 utils.py:305(<listcomp>)\n",
       "        1    0.000    0.000    0.004    0.004 <string>:1(<module>)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'split' of '_sre.SRE_Pattern' objects}\n",
       "        3    0.000    0.000    0.000    0.000 <ipython-input-10-20eebb3b12a0>:1(pad_sentences)\n",
       "       45    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "        9    0.000    0.000    0.000    0.000 {method 'finditer' of '_sre.SRE_Pattern' objects}\n",
       "       32    0.000    0.000    0.000    0.000 {method 'group' of '_sre.SRE_Match' objects}\n",
       "        9    0.000    0.000    0.000    0.000 utils.py:339(any2unicode)\n",
       "       30    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
       "        9    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
       "       13    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun generate_sequence_tf(texts, 10, punct_re, vocabulary=dict_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         447 function calls (440 primitive calls) in 0.017 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.004    0.004    0.017    0.017 <string>:1(<module>)\n",
       "      2/1    0.003    0.002    0.004    0.004 <ipython-input-46-59e98ab18601>:23(generate_sequence_tf_numba)\n",
       "        7    0.002    0.000    0.009    0.001 dispatcher.py:420(typeof_pyval)\n",
       "        4    0.002    0.000    0.004    0.001 abstract.py:54(__call__)\n",
       "        7    0.001    0.000    0.007    0.001 typeof.py:22(typeof)\n",
       "        5    0.001    0.000    0.002    0.000 typeof.py:35(typeof_impl)\n",
       "        5    0.001    0.000    0.001    0.000 typeof.py:56(_typeof_buffer)\n",
       "      6/4    0.001    0.000    0.001    0.000 abstract.py:110(__hash__)\n",
       "        5    0.000    0.000    0.001    0.000 api.py:178(typeof)\n",
       "        4    0.000    0.000    0.001    0.000 {method 'get' of 'dict' objects}\n",
       "        4    0.000    0.000    0.002    0.000 abstract.py:42(_intern)\n",
       "        2    0.000    0.000    0.000    0.000 abstract.py:88(key)\n",
       "        2    0.000    0.000    0.000    0.000 containers.py:318(__init__)\n",
       "        5    0.000    0.000    0.001    0.000 cffi_utils.py:37(is_cffi_func)\n",
       "        2    0.000    0.000    0.004    0.002 typeof.py:147(_typeof_list)\n",
       "        9    0.000    0.000    0.000    0.000 weakref.py:393(__getitem__)\n",
       "      9/7    0.000    0.000    0.006    0.001 functools.py:802(wrapper)\n",
       "        9    0.000    0.000    0.000    0.000 functools.py:764(dispatch)\n",
       "        2    0.000    0.000    0.000    0.000 misc.py:68(__init__)\n",
       "        5    0.000    0.000    0.000    0.000 cffi_utils.py:27(is_ffi_instance)\n",
       "       41    0.000    0.000    0.000    0.000 utils.py:265(simple_tokenize)\n",
       "       10    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "        2    0.000    0.000    0.000    0.000 typeof.py:124(_typeof_str)\n",
       "        2    0.000    0.000    0.001    0.000 dispatcher.py:35(_numba_type_)\n",
       "        7    0.000    0.000    0.000    0.000 <string>:12(__new__)\n",
       "        2    0.000    0.000    0.000    0.000 abstract.py:107(__repr__)\n",
       "       10    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:997(_handle_fromlist)\n",
       "        1    0.000    0.000    0.017    0.017 {built-in method builtins.exec}\n",
       "       21    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        9    0.000    0.000    0.000    0.000 utils.py:223(tokenize)\n",
       "        6    0.000    0.000    0.000    0.000 misc.py:72(key)\n",
       "        4    0.000    0.000    0.000    0.000 abstract.py:85(__init__)\n",
       "        9    0.000    0.000    0.000    0.000 utils.py:305(<listcomp>)\n",
       "       45    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        9    0.000    0.000    0.000    0.000 utils.py:283(simple_preprocess)\n",
       "        4    0.000    0.000    0.000    0.000 abstract.py:113(__eq__)\n",
       "        6    0.000    0.000    0.000    0.000 containers.py:339(key)\n",
       "       30    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
       "        7    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x1076e5dd8}\n",
       "        9    0.000    0.000    0.000    0.000 {method 'finditer' of '_sre.SRE_Pattern' objects}\n",
       "      6/4    0.000    0.000    0.001    0.000 {built-in method builtins.hash}\n",
       "       37    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "        9    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
       "       10    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
       "        5    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "        9    0.000    0.000    0.000    0.000 utils.py:339(any2unicode)\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "       32    0.000    0.000    0.000    0.000 {method 'group' of '_sre.SRE_Match' objects}\n",
       "        3    0.000    0.000    0.000    0.000 <ipython-input-10-20eebb3b12a0>:1(pad_sentences)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun generate_sequence_tf_numba(texts, 10, punct_re, vocabulary=dict_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 2., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [2., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sequence_idf(texts, max_len, punct_re, preprocess = simple_preprocess, vocabulary=None, window=0):\n",
    "    idf_vec = np.zeros((len(vocabulary),max_len))\n",
    "    for text in texts:\n",
    "        combinations = []\n",
    "        for split_text in punct_re.split(text):\n",
    "            processed_text = preprocess(split_text)\n",
    "            l = len(processed_text)\n",
    "            if l <= max_len and l != 0:\n",
    "                combinations += [[vocabulary.index(word),seq] for seq, word in enumerate(processed_text)]\n",
    "                \n",
    "            else: # need to check its behaviour\n",
    "                for padded_text in pad_sentences(processed_text, max_len, window=window):\n",
    "                    combinations += [(vocabulary.index(word),seq) for seq, word in enumerate(padded_text)]\n",
    "        combinations = np.unique(combinations,axis=0)\n",
    "        idf_vec[combinations[:,0],combinations[:,1]] += 1 \n",
    "    return idf_vec\n",
    "\n",
    "@numba.jit\n",
    "def generate_sequence_idf_numba(texts, max_len, punct_re, preprocess = simple_preprocess, vocabulary=None, window=0):\n",
    "    idf_vec = np.zeros((len(vocabulary),max_len))\n",
    "    for text in texts:\n",
    "        combinations = []\n",
    "        for split_text in punct_re.split(text):\n",
    "            processed_text = preprocess(split_text)\n",
    "            l = len(processed_text)\n",
    "            if l <= max_len and l != 0:\n",
    "                for seq, word in enumerate(processed_text):\n",
    "                    combinations.append([vocabulary.index(word),seq])\n",
    "#                 combinations += [[vocabulary.index(word),seq] for seq, word in enumerate(processed_text)]\n",
    "                \n",
    "            else: # need to check its behaviour\n",
    "                for padded_text in pad_sentences(processed_text, max_len, window=window):\n",
    "                    for seq, word in enumerate(processed_text):\n",
    "                        combinations.append([vocabulary.index(word),seq])\n",
    "        combinations = np.unique(combinations,axis=0)\n",
    "        idf_vec[combinations[:,0],combinations[:,1]] += 1 \n",
    "    return idf_vec\n",
    "\n",
    "\n",
    "# generate_sequence_idf(texts, 10, punct_re, vocabulary=vocab)\n",
    "# generate_sequence_idf(texts, 10, punct_re, vocabulary=vocab).shape\n",
    "generate_sequence_idf_numba(texts, 10, punct_re, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 µs ± 6.45 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_sequence_idf_numba(texts, 10, punct_re, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231 µs ± 2.26 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_sequence_idf(texts, 10, punct_re, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.200004  , 0.31080229, 0.4282458 , 0.54326407, 0.64992798])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp =np.arange(0,1,0.1).reshape(2,5)\n",
    "# print(inp)\n",
    "# inp.sum(axis=0)\n",
    "def squashing_func_A(vectors,eps=1e-5):\n",
    "    s_squared_norm = np.sum(np.square(vectors))\n",
    "#     print(s_squared_norm)\n",
    "    scale = np.sqrt(s_squared_norm + eps)\n",
    "    return (s_squared_norm/(1+s_squared_norm))*(vectors / scale)\n",
    "# squashing_func_A(inp)\n",
    "# squashing_func_A(inp).sum(axis=0)\n",
    "\n",
    "def squashing_func_B(vectors, dim=-1, eps=1e-5,return_normed=False):\n",
    "    norm2 = np.sum(np.square(vectors), axis=dim)\n",
    "    coeff = (np.sqrt(norm2+eps) / (1.0 + norm2))\n",
    "    if return_normed:\n",
    "        return (np.sqrt(norm2+eps) / (1.0 + norm2)) * vectors\n",
    "    return coeff * np.sum(vectors,axis=dim)\n",
    "# squashing_func_B(inp,dim=0,return_normed=True)\n",
    "squashing_func_B(inp,dim=0)\n",
    "# squashing_func_B(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.64444444, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 3., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index('are')\n",
    "generate_sequence_tf(texts, 10, punct_re, vocabulary=vocab)[vocab.index('are'),:]\n",
    "generate_sequence_idf(texts, 10, punct_re, vocabulary=vocab)[vocab.index('are'),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sequence_tf_idf(texts, max_len, punct_re, preprocess = simple_preprocess, vocabulary=None, window=0):\n",
    "    idf_vec = np.zeros((len(vocabulary),max_len))\n",
    "    tf_vec = np.zeros((len(vocabulary),max_len))\n",
    "    vocab_is_dict = isinstance(vocabulary,dict)\n",
    "    for text in texts:\n",
    "        combinations = []\n",
    "        for split_text in punct_re.split(text):\n",
    "            processed_text = preprocess(split_text)\n",
    "            l = len(processed_text)\n",
    "            if l <= max_len and l != 0:\n",
    "                for seq, word in enumerate(processed_text):\n",
    "                    indx = vocabulary[word] if vocab_is_dict else vocabulary.index(word)\n",
    "                    combinations.append([indx,seq])\n",
    "                    tf_vec[indx,seq] += 1/l\n",
    "                \n",
    "            else: # need to check its behaviour\n",
    "                for padded_text in pad_sentences(processed_text, max_len, window=window):\n",
    "                    for seq, word in enumerate(processed_text):\n",
    "                        indx = vocabulary[word] if vocab_is_dict else vocabulary.index(word)\n",
    "                        combinations.append([indx,seq])\n",
    "                        tf_vec[indx,seq] += 1/l\n",
    "        \n",
    "        combinations = np.unique(combinations,axis=0)\n",
    "        idf_vec[combinations[:,0],combinations[:,1]] += 1 \n",
    "    return tf_vec,idf_vec\n",
    "\n",
    "@numba.jit\n",
    "def generate_sequence_tf_idf_numba(texts, max_len, punct_re, preprocess = simple_preprocess, vocabulary=None, window=0):\n",
    "    idf_vec = np.zeros((len(vocabulary),max_len))\n",
    "    tf_vec = np.zeros((len(vocabulary),max_len))\n",
    "    vocab_is_dict = isinstance(vocabulary,dict)\n",
    "    for text in texts:\n",
    "        combinations = []\n",
    "        for split_text in punct_re.split(text):\n",
    "            processed_text = preprocess(split_text)\n",
    "            l = len(processed_text)\n",
    "            if l <= max_len and l != 0:\n",
    "                for seq, word in enumerate(processed_text):\n",
    "                    indx = vocabulary[word] if vocab_is_dict else vocabulary.index(word)\n",
    "                    combinations.append([indx,seq])\n",
    "                    tf_vec[indx,seq] += 1/l\n",
    "                \n",
    "            else: # need to check its behaviour\n",
    "                for padded_text in pad_sentences(processed_text, max_len, window=window):\n",
    "                    for seq, word in enumerate(processed_text):\n",
    "                        indx = vocabulary[word] if vocab_is_dict else vocabulary.index(word)\n",
    "                        combinations.append([indx,seq])\n",
    "                        tf_vec[indx,seq] += 1/l\n",
    "        \n",
    "        combinations = np.unique(combinations,axis=0)\n",
    "        idf_vec[combinations[:,0],combinations[:,1]] += 1 \n",
    "    return tf_vec,idf_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646 µs ± 12.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_sequence_tf_idf_numba(texts, 10000, punct_re, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426 µs ± 3.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_sequence_tf_idf(texts, 10000, punct_re, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what 23 0 0.5333333333333333 2.0\n",
      "are 1 1 0.3111111111111111 2.0\n",
      "capsules 3 2 0.0 0.0\n",
      "lets 9 0 0.25 1.0\n",
      "read 13 1 0.25 1.0\n",
      "about 0 2 0.25 1.0\n",
      "it 7 3 0.25 1.0\n",
      "[[0.21624806 0.1261447  1.09861229 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.27465307 0.27465307 0.27465307 0.27465307 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06736462, 0.03493628, 0.54508784, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.08555869, 0.07606627, 0.13627196, 0.07014776, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what 23 0 0.5333333333333333 2.0\n",
      "are 1 1 0.3111111111111111 2.0\n",
      "capsules 3 2 0.0 0.0\n",
      "lets 9 0 0.25 1.0\n",
      "read 13 1 0.25 1.0\n",
      "about 0 2 0.25 1.0\n",
      "it 7 3 0.25 1.0\n",
      "[[0.21624806 0.1261447  1.09861229 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.27465307 0.27465307 0.27465307 0.27465307 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.15292332, 0.11100255, 0.6813598 , 0.07014776, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_capsule_vectors(text, tf_vec, idf_vec, punct_re, num_docs, preprocess =simple_preprocess, vocabulary=None, window=0, return_normed=False):\n",
    "    splits = punct_re.split(text)\n",
    "    max_len = int(idf_vec.shape[1])\n",
    "    capsules = np.zeros((len(splits),max_len))\n",
    "    for cap_indx, split_text in enumerate(splits):\n",
    "        processed_text = preprocess(split_text)\n",
    "        l = len(processed_text)\n",
    "        if l <= max_len and l != 0:\n",
    "            for seq, word in enumerate(processed_text):\n",
    "                indx = vocabulary.index(word)\n",
    "                print(word, indx, seq, tf_vec[indx,seq], idf_vec[indx,seq])\n",
    "                capsules[cap_indx,seq] += (tf_vec[indx,seq] * np.log(num_docs/idf_vec[indx,seq]) if idf_vec[indx,seq] != 0 else np.log(num_docs))\n",
    "            \n",
    "        else: # need to check its behaviour\n",
    "            for padded_text in pad_sentences(processed_text, max_len, window=window):\n",
    "                for seq, word in enumerate(padded_text):\n",
    "                    indx = vocabulary.index(word)\n",
    "                    capsules[cap_indx,seq] += tf_vec[indx,seq] * np.log(num_docs/idf_vec[indx,seq])\n",
    "    print(capsules)            \n",
    "    return squashing_func_B(capsules,dim=0,return_normed=return_normed)\n",
    "\n",
    "get_capsule_vectors('What are capsules? Lets read about it.',\n",
    "                    generate_sequence_tf(texts, 10, punct_re, vocabulary=vocab),\n",
    "                    generate_sequence_idf(texts, 10, punct_re, vocabulary=vocab),\n",
    "                    punct_re=punct_re, num_docs=3, vocabulary=vocab,return_normed=True)\n",
    "\n",
    "get_capsule_vectors('What are capsules? Lets read about it.',\n",
    "                    generate_sequence_tf(texts, 10, punct_re, vocabulary=vocab),\n",
    "                    generate_sequence_idf(texts, 10, punct_re, vocabulary=vocab),\n",
    "                    punct_re=punct_re, num_docs=3, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentCapsule():\n",
    "    \n",
    "    def __init__(self, feature_len=100, sequence_tf=True, sequence_idf=True, sentence_splitter='?!.',\n",
    "                 pad_senetence_window=0, pad_value='global_0', compressor='squash', dictionary=None, tokenizer=None,\n",
    "                 preprocess = simple_preprocess, stopwords=None, epsilon=1e-6,return_normed=False):\n",
    "        \n",
    "        self.feature_len = feature_len\n",
    "        self.sequence_tf = sequence_tf\n",
    "        self.sequence_idf = sequence_idf\n",
    "        self.sentence_splitter = sentence_splitter\n",
    "        self.pad_senetence_window = pad_senetence_window\n",
    "        self.pad_value = pad_value\n",
    "        self.compressor = 'squash'\n",
    "        self.preprocess = preprocess\n",
    "        self.dictionary = dictionary\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stopwords = stopwords\n",
    "        self.eps = epsilon\n",
    "        self.return_normed = return_normed\n",
    "        \n",
    "        self.preprocessor = self.tokenizer if tokenizer is not None else self.preprocess\n",
    "        self.tf_vectors = None\n",
    "        self.idf_vectors = None\n",
    "        self.num_docs = None\n",
    "        self.vocabulary = None\n",
    "        \n",
    "        self.sentence_splitter_re = re.compile('\\?|\\!|\\.',re.UNICODE)\n",
    "        \n",
    "    def _squash(self, vectors, dim=-1, return_normed=False):\n",
    "        norm2 = np.sum(np.square(vectors), axis=dim)\n",
    "        coeff = (np.sqrt(norm2+self.eps) / (1.0 + norm2))\n",
    "        if return_normed:\n",
    "            return (np.sqrt(norm2+eps) / (1.0 + norm2)) * vectors\n",
    "        return coeff * np.sum(vectors,axis=dim)\n",
    "    \n",
    "    @numba.jit\n",
    "    def _generate_sequence_tf(self, texts):\n",
    "        \n",
    "        preprocess= self.preprocessor\n",
    "        max_len = self.feature_len\n",
    "        punct_re = self.sentence_splitter_re\n",
    "        vocabulary = self.vocabulary\n",
    "        window = self.pad_senetence_window\n",
    "        \n",
    "        tf_vec = np.zeros((len(vocabulary),max_len))\n",
    "        for text in texts:\n",
    "            for split_text in punct_re.split(text):\n",
    "                processed_text = preprocess(split_text)\n",
    "                l = len(processed_text)\n",
    "\n",
    "                if l <= max_len and l != 0:\n",
    "                    for seq, word in enumerate(processed_text):\n",
    "                        indx = vocabulary.index(word)\n",
    "                        tf_vec[indx,seq] += 1/l\n",
    "\n",
    "                else: # need to check its behaviour\n",
    "                    for padded_text in pad_sentences(processed_text, max_len, window=window):\n",
    "                        for seq, word in enumerate(padded_text):\n",
    "                            indx = vocabulary.index(word)\n",
    "                            tf_vec[indx,seq] += 1\n",
    "\n",
    "        return tf_vec\n",
    "    \n",
    "#     @numba.jit\n",
    "    def _generate_sequence_idf(self, texts):\n",
    "        \n",
    "        preprocess= self.preprocessor\n",
    "        max_len = self.feature_len\n",
    "        punct_re = self.sentence_splitter_re\n",
    "        vocabulary = self.vocabulary\n",
    "        window = self.pad_senetence_window\n",
    "        idf_vec = np.zeros((len(vocabulary),max_len))\n",
    "\n",
    "        for text in texts:\n",
    "            combinations = []\n",
    "            for split_text in punct_re.split(text):\n",
    "                processed_text = preprocess(split_text)\n",
    "                l = len(processed_text)\n",
    "                if l <= max_len and l != 0:\n",
    "                    combinations += [[vocabulary.index(word),seq] for seq, word in enumerate(processed_text)]\n",
    "\n",
    "                else: # need to check its behaviour\n",
    "                    for padded_text in pad_sentences(processed_text, max_len, window=window):\n",
    "                        combinations += [(vocabulary.index(word),seq) for seq, word in enumerate(padded_text)]\n",
    "            combinations = np.unique(combinations,axis=0)\n",
    "            idf_vec[combinations[:,0],combinations[:,1]] += 1 \n",
    "        return idf_vec\n",
    "    \n",
    "    def _generate_vocabulary(self, texts):\n",
    "        vocab = []\n",
    "        for text in texts:\n",
    "            for token in self.preprocessor(text):\n",
    "                vocab.append(token)\n",
    "        return np.unique(vocab).tolist()\n",
    "    \n",
    "    @numba.jit\n",
    "    def _get_capsule_vectors(self, text):\n",
    "        \n",
    "        preprocess= self.preprocessor\n",
    "        max_len = self.feature_len\n",
    "        vocabulary = self.vocabulary\n",
    "        window = self.pad_senetence_window\n",
    "        return_normed = self.return_normed\n",
    "        tf_vec = self.tf_vectors\n",
    "        idf_vec = self.idf_vectors\n",
    "        num_docs = self.num_docs \n",
    "        splits = self.sentence_splitter_re.split(text)\n",
    "        capsules = np.zeros((len(splits),max_len))\n",
    "        \n",
    "        for cap_indx, split_text in enumerate(splits):\n",
    "            processed_text = preprocess(split_text)\n",
    "            l = len(processed_text)\n",
    "            if l <= max_len and l != 0:\n",
    "                for seq, word in enumerate(processed_text):\n",
    "                    indx = vocabulary.index(word)\n",
    "                    #print(word, indx, seq, tf_vec[indx,seq], idf_vec[indx,seq])\n",
    "                    capsules[cap_indx,seq] += (tf_vec[indx,seq] * np.log(num_docs/idf_vec[indx,seq]) if idf_vec[indx,seq] != 0 else np.log(num_docs))\n",
    "\n",
    "            else: # need to check its behaviour\n",
    "                for padded_text in pad_sentences(processed_text, max_len, window=window):\n",
    "                    for seq, word in enumerate(padded_text):\n",
    "                        indx = vocabulary.index(word)\n",
    "                        capsules[cap_indx,seq] += tf_vec[indx,seq] * np.log(num_docs/idf_vec[indx,seq])\n",
    "        \n",
    "        # print(capsules)            \n",
    "        return self._squash(capsules, dim=0, return_normed=return_normed)\n",
    "        \n",
    "    def fit(self,X):\n",
    "        \n",
    "        if self.dictionary is None:\n",
    "            self.vocabulary = self._generate_vocabulary(X)\n",
    "\n",
    "        if self.sequence_tf:\n",
    "            self.tf_vectors = self._generate_sequence_tf(X)\n",
    "        else:\n",
    "            self.tf_vectors= np.ones((len(self.vocabulary),self.feature_len))\n",
    "        \n",
    "        if self.sequence_idf:\n",
    "            self.idf_vectors = self._generate_sequence_idf(X)\n",
    "        else:\n",
    "            self.idf_vectors = np.ones((len(self.vocabulary),self.feature_len))\n",
    "            \n",
    "        self.num_docs = len(X)\n",
    "        \n",
    "    def transform(self,X):\n",
    "        num_docs_to_transform = len(X)\n",
    "        transform_capsule = np.zeros((num_docs_to_transform,self.feature_len))\n",
    "        for idx,text in enumerate(X):\n",
    "            transform_capsule[idx,:] = self._get_capsule_vectors(text)\n",
    "        return transform_capsule\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doccapsule = DocumentCapsule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doccapsule.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15291769, 0.11099708, 0.68135741, 0.07014358, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doccapsule.transform(['What are capsules? Lets read about it.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hyper]",
   "language": "python",
   "name": "conda-env-hyper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
